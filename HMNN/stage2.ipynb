{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7238952b7696fd47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T04:26:16.962891400Z",
     "start_time": "2023-12-11T04:26:10.746746600Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import math\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import get_train_test_data2, MMoE_Dataset,get_train_test_data1\n",
    "from models import MMoE_Model, Transfer_Model, MMoE_Model_Expert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d496c19b2a42fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T04:26:24.452099900Z",
     "start_time": "2023-12-11T04:26:24.447572300Z"
    }
   },
   "outputs": [],
   "source": [
    "file_path = \"./data/mutilsolvent2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f48733c9963a03b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T04:26:25.207805200Z",
     "start_time": "2023-12-11T04:26:25.184428300Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957f3fe4a2c9745b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T04:26:25.742973200Z",
     "start_time": "2023-12-11T04:26:25.718114700Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_train_test_data(file_path, test_size,seed,dataset_num):\n",
    "    data = pd.read_csv(file_path).values\n",
    "    x_train_list=[]\n",
    "    y_train_list=[]\n",
    "    x_test_list=[]\n",
    "    y_test_list=[]\n",
    "    for i in range(15):\n",
    "        if i == dataset_num:\n",
    "            cur_data = data[35*i:35*(i+1),:]\n",
    "            transfer_x,transfer_y = cur_data[:, :-4], cur_data[:, -4:]\n",
    "        else:\n",
    "            cur_data = data[35*i:35*(i+1),:]\n",
    "            X, y = cur_data[:, :-4], cur_data[:, -4:]\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size,random_state=seed)\n",
    "            x_train_list.append(x_train)\n",
    "            y_train_list.append(y_train)\n",
    "            x_test_list.append(x_test)\n",
    "            y_test_list.append(y_test)\n",
    "    x_train, x_test, y_train, y_test = np.concatenate(x_train_list),np.concatenate(x_test_list),np.concatenate(y_train_list),np.concatenate(y_test_list)\n",
    "    print(f\"dataset num {dataset_num}\")\n",
    "    print(f\"the number of train samples is: {len(x_train)}\")\n",
    "    print(f\"the number of test samples is: {len(x_test)}\")\n",
    "    print(f\"the number of transfer samples is: {len(transfer_x)}\")\n",
    "    return x_train, x_test, y_train, y_test,transfer_x,transfer_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46db1b0481f6abea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T04:26:26.851521800Z",
     "start_time": "2023-12-11T04:26:26.671201400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset num 0\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 1\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 2\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 3\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 4\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 5\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 6\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 7\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 8\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 9\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 10\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 11\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 12\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 13\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n",
      "dataset num 14\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    x_train, x_test, y_train, y_test,transfer_X,transfer_y = get_train_test_data(file_path,0.2,42,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82dc5a71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-11T10:19:49.778738600Z",
     "start_time": "2023-12-11T04:27:05.728633600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(file_path='data/mutilsolvent2.csv', model_save_dir='./p2results_1', res_dir='./p2results_1', water_model_save_path='water_model_result/waterdi6zhe6.ckpt', air_model_save_path='air2_model_result/air2di9zhe6.ckpt', water_select_expert_config='water_res_result/waterdi6zhe6_top2.csv', air_select_expert_config='air2_res_result/air2di9zhe6_top2.csv', input_dim=19, represent_dim=100, pair_embedding_dim=5, expert_num=6, gate_output_dim=10, epochs=2000, early_stop_num=200, lr=0.0005, verbose=False, writer_flag=False, batch_size=16, test_size=0.2, seed=42, eps='', frozen=False)\n",
      "\n",
      "\n",
      "\n",
      "====start to train mutilsolvent2.csv====\n",
      "data/mutilsolvent2.csv\n",
      "dataset num 0\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1446/2000 [27:02<10:21,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model is not improving, so we halt the training session at epoch: 1447.\n",
      "\n",
      " Best Model loss_total: -35.178552, loss1: 0.000011,loss2: 0.000004,loss3: 0.000055,loss4: 0.000002\n",
      "\n",
      "theta1:Parameter containing:\n",
      "tensor([0.0040], requires_grad=True),theta2:Parameter containing:\n",
      "tensor([0.0051], requires_grad=True),theta3:Parameter containing:\n",
      "tensor([0.2916], requires_grad=True),theta4:Parameter containing:\n",
      "tensor([0.0067], requires_grad=True)\n",
      "file:mutilsolvent2.csv, rmse:[0.029797256466409593, 0.027706469513326656, 0.047749076298665795, 0.013778083467012653], r2:[0.42629488504841695, -0.9291841068984643, 0.9088742659154897, 0.6984342601844974],mae:[0.020375688, 0.023405433, 0.03256348, 0.0109061245],mape:[0.22466406, 0.24779895, 0.0031265172, 0.0026409042]\n",
      "dataset num 1\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [37:07<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train all epochs.\n",
      "\n",
      " Best Model loss_total: -33.956520, loss1: 0.000014,loss2: 0.000009,loss3: 0.000051,loss4: 0.000002\n",
      "\n",
      "theta1:Parameter containing:\n",
      "tensor([0.0121], requires_grad=True),theta2:Parameter containing:\n",
      "tensor([0.0034], requires_grad=True),theta3:Parameter containing:\n",
      "tensor([0.2957], requires_grad=True),theta4:Parameter containing:\n",
      "tensor([0.0085], requires_grad=True)\n",
      "file:mutilsolvent2.csv, rmse:[0.022720171669691522, 0.02347534943182242, 0.04170875484945548, 0.01410989174495252], r2:[0.6643913903356986, -0.3017820459827456, 0.9300480981733357, 0.6850573062643673],mae:[0.014975544, 0.018121976, 0.033596337, 0.010045433],mape:[0.15966433, 0.18220586, 0.0032501195, 0.0024338122]\n",
      "dataset num 2\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 1723/2000 [32:25<05:12,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model is not improving, so we halt the training session at epoch: 1724.\n",
      "\n",
      " Best Model loss_total: -33.604236, loss1: 0.000013,loss2: 0.000018,loss3: 0.000051,loss4: 0.000002\n",
      "\n",
      "theta1:Parameter containing:\n",
      "tensor([0.0068], requires_grad=True),theta2:Parameter containing:\n",
      "tensor([0.0028], requires_grad=True),theta3:Parameter containing:\n",
      "tensor([0.3003], requires_grad=True),theta4:Parameter containing:\n",
      "tensor([0.0079], requires_grad=True)\n",
      "file:mutilsolvent2.csv, rmse:[0.012098727789706267, 0.006535238888235723, 0.03546490381357397, 0.005392506200614455], r2:[0.9035216611997259, 0.9130249487921691, 0.9482020010070202, 0.9543460849246509],mae:[0.0079994835, 0.0040275836, 0.017980222, 0.0031792095],mape:[0.0861511, 0.045509003, 0.0017565632, 0.00076915044]\n",
      "dataset num 3\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 1804/2000 [36:02<03:54,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model is not improving, so we halt the training session at epoch: 1805.\n",
      "\n",
      " Best Model loss_total: -33.589671, loss1: 0.000015,loss2: 0.000010,loss3: 0.000069,loss4: 0.000003\n",
      "\n",
      "theta1:Parameter containing:\n",
      "tensor([0.0044], requires_grad=True),theta2:Parameter containing:\n",
      "tensor([0.0041], requires_grad=True),theta3:Parameter containing:\n",
      "tensor([0.3260], requires_grad=True),theta4:Parameter containing:\n",
      "tensor([0.0019], requires_grad=True)\n",
      "file:mutilsolvent2.csv, rmse:[0.007736746122104785, 0.0022092849965581708, 0.01768904778892312, 0.0014405037462615071], r2:[0.9603858744591883, 0.990535220650693, 0.9869400374729097, 0.9967481467637404],mae:[0.005054915, 0.0017535661, 0.007886124, 0.0011199134],mape:[0.07304323, 0.017531581, 0.000759931, 0.0002716964]\n",
      "dataset num 4\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 71%|███████   | 1424/2000 [27:01<10:55,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model is not improving, so we halt the training session at epoch: 1425.\n",
      "\n",
      " Best Model loss_total: -31.997020, loss1: 0.000020,loss2: 0.000017,loss3: 0.000069,loss4: 0.000005\n",
      "\n",
      "theta1:Parameter containing:\n",
      "tensor([0.0071], requires_grad=True),theta2:Parameter containing:\n",
      "tensor([0.0041], requires_grad=True),theta3:Parameter containing:\n",
      "tensor([0.3486], requires_grad=True),theta4:Parameter containing:\n",
      "tensor([0.0066], requires_grad=True)\n",
      "file:mutilsolvent2.csv, rmse:[0.006211838901422191, 0.002726480066688304, 0.010331299609192731, 0.0022738362750381706], r2:[0.9744317765041465, 0.9858871201410792, 0.9955121769309814, 0.9918977684127958],mae:[0.004861713, 0.002370181, 0.00749103, 0.0016605649],mape:[0.07209461, 0.022093987, 0.00072499714, 0.00040308692]\n",
      "dataset num 5\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 1904/2000 [36:18<01:49,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model is not improving, so we halt the training session at epoch: 1905.\n",
      "\n",
      " Best Model loss_total: -33.857360, loss1: 0.000019,loss2: 0.000009,loss3: 0.000055,loss4: 0.000003\n",
      "\n",
      "theta1:Parameter containing:\n",
      "tensor([0.0047], requires_grad=True),theta2:Parameter containing:\n",
      "tensor([0.0034], requires_grad=True),theta3:Parameter containing:\n",
      "tensor([0.3123], requires_grad=True),theta4:Parameter containing:\n",
      "tensor([0.0023], requires_grad=True)\n",
      "file:mutilsolvent2.csv, rmse:[0.009689312281552849, 0.0017306890545264186, 0.0066665005369736, 0.0009599120764195492], r2:[0.9377191402237156, 0.9944227189958422, 0.9981169288932077, 0.9985562903887922],mae:[0.0056530284, 0.0014596609, 0.004766192, 0.00066145486],mape:[0.07381285, 0.013178091, 0.00046168736, 0.00016061835]\n",
      "dataset num 6\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [37:34<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train all epochs.\n",
      "\n",
      " Best Model loss_total: -33.988636, loss1: 0.000014,loss2: 0.000010,loss3: 0.000040,loss4: 0.000003\n",
      "\n",
      "theta1:Parameter containing:\n",
      "tensor([0.0061], requires_grad=True),theta2:Parameter containing:\n",
      "tensor([0.0043], requires_grad=True),theta3:Parameter containing:\n",
      "tensor([0.3194], requires_grad=True),theta4:Parameter containing:\n",
      "tensor([0.0021], requires_grad=True)\n",
      "file:mutilsolvent2.csv, rmse:[0.006973438761649786, 0.0029004374983059235, 0.012681371686909087, 0.0014490266873070502], r2:[0.9677045079357551, 0.9847658915425318, 0.9930944545276742, 0.996721669362682],mae:[0.0039751097, 0.0023368606, 0.007573182, 0.0012609345],mape:[0.056241944, 0.022118488, 0.0007346187, 0.00030636636]\n",
      "dataset num 7\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 1184/2000 [22:35<15:33,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model is not improving, so we halt the training session at epoch: 1185.\n",
      "\n",
      " Best Model loss_total: -31.265006, loss1: 0.000030,loss2: 0.000019,loss3: 0.000085,loss4: 0.000007\n",
      "\n",
      "theta1:Parameter containing:\n",
      "tensor([0.0090], requires_grad=True),theta2:Parameter containing:\n",
      "tensor([0.0056], requires_grad=True),theta3:Parameter containing:\n",
      "tensor([0.4462], requires_grad=True),theta4:Parameter containing:\n",
      "tensor([0.0028], requires_grad=True)\n",
      "file:mutilsolvent2.csv, rmse:[0.005095924777288037, 0.002064920286436246, 0.007677636688961552, 0.0008209932260656174], r2:[0.9827494684465995, 0.9923362283751231, 0.9974586180765858, 0.9989467552889559],mae:[0.0040921206, 0.0016941056, 0.0056277686, 0.00066526956],mape:[0.054959577, 0.015361971, 0.00054409815, 0.00016164305]\n",
      "dataset num 8\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [37:46<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train all epochs.\n",
      "\n",
      " Best Model loss_total: -33.639111, loss1: 0.000012,loss2: 0.000011,loss3: 0.000043,loss4: 0.000002\n",
      "\n",
      "theta1:Parameter containing:\n",
      "tensor([0.0081], requires_grad=True),theta2:Parameter containing:\n",
      "tensor([0.0047], requires_grad=True),theta3:Parameter containing:\n",
      "tensor([0.3058], requires_grad=True),theta4:Parameter containing:\n",
      "tensor([0.0057], requires_grad=True)\n",
      "file:mutilsolvent2.csv, rmse:[0.005501474114843421, 0.001139148152777673, 0.0061639423117279225, 0.0005455299214673627], r2:[0.979911467498803, 0.9976810015533774, 0.9983564764185289, 0.9995343659416923],mae:[0.003228799, 0.0008769755, 0.004623113, 0.00042834962],mape:[0.036959816, 0.0077439216, 0.00044841575, 0.00010368959]\n",
      "dataset num 9\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 1418/2000 [26:16<10:46,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model is not improving, so we halt the training session at epoch: 1419.\n",
      "\n",
      " Best Model loss_total: -32.136572, loss1: 0.000023,loss2: 0.000015,loss3: 0.000116,loss4: 0.000004\n",
      "\n",
      "theta1:Parameter containing:\n",
      "tensor([0.0060], requires_grad=True),theta2:Parameter containing:\n",
      "tensor([0.0048], requires_grad=True),theta3:Parameter containing:\n",
      "tensor([0.3820], requires_grad=True),theta4:Parameter containing:\n",
      "tensor([0.0038], requires_grad=True)\n",
      "file:mutilsolvent2.csv, rmse:[0.006864359323895021, 0.0013745961924383277, 0.01155081072408633, 0.001348763719774982], r2:[0.968668979950203, 0.9966563141723516, 0.9941943145747837, 0.9971564657679043],mae:[0.0041748993, 0.0010106879, 0.0060543334, 0.0011870384],mape:[0.051009785, 0.008835585, 0.0005835787, 0.00028753793]\n",
      "dataset num 10\n",
      "the number of train samples is: 392\n",
      "the number of test samples is: 98\n",
      "the number of transfer samples is: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1749/2000 [32:31<04:39,  1.12s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 233\u001B[0m\n\u001B[0;32m    229\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset, karg\u001B[38;5;241m.\u001B[39mbatch_size, shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, pin_memory\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    230\u001B[0m model \u001B[38;5;241m=\u001B[39m Transfer_Model(input_dim\u001B[38;5;241m=\u001B[39mkarg\u001B[38;5;241m.\u001B[39minput_dim, represent_dim\u001B[38;5;241m=\u001B[39mkarg\u001B[38;5;241m.\u001B[39mrepresent_dim, pair_embedding_dim\u001B[38;5;241m=\u001B[39mkarg\u001B[38;5;241m.\u001B[39mpair_embedding_dim,\n\u001B[0;32m    231\u001B[0m                    expert_num\u001B[38;5;241m=\u001B[39mkarg\u001B[38;5;241m.\u001B[39mexpert_num,gate_output_dim\u001B[38;5;241m=\u001B[39mkarg\u001B[38;5;241m.\u001B[39mgate_output_dim)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m--> 233\u001B[0m trainer(model_water,water_select_expert,model_air,air_select_expert,train_dataloader, model, model_save_path, device, karg\u001B[38;5;241m.\u001B[39mlr, karg\u001B[38;5;241m.\u001B[39mepochs, karg\u001B[38;5;241m.\u001B[39mearly_stop_num, karg\u001B[38;5;241m.\u001B[39mverbose, karg\u001B[38;5;241m.\u001B[39mwriter_flag)\n\u001B[0;32m    235\u001B[0m water_expert_represents_test \u001B[38;5;241m=\u001B[39m model_water(torch\u001B[38;5;241m.\u001B[39mTensor(transfer_X)\u001B[38;5;241m.\u001B[39mto(device))[:, water_select_expert, :]\n\u001B[0;32m    236\u001B[0m air_expert_represents_test \u001B[38;5;241m=\u001B[39m model_air(torch\u001B[38;5;241m.\u001B[39mTensor(transfer_X)\u001B[38;5;241m.\u001B[39mto(device))[:, air_select_expert, :]\n",
      "Cell \u001B[1;32mIn[7], line 60\u001B[0m, in \u001B[0;36mtrainer\u001B[1;34m(water_model, water_select_expert, air_model, air_select_expert, train_loader, model, model_save_path, device, lr, epochs, early_stop_num, verbose, writer_flag)\u001B[0m\n\u001B[0;32m     58\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     59\u001B[0m loss_total\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m---> 60\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     61\u001B[0m step \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     63\u001B[0m loss_total_record\u001B[38;5;241m.\u001B[39mappend(loss_total\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mitem())\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:373\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    368\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    369\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[0;32m    370\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresult\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    371\u001B[0m             )\n\u001B[1;32m--> 373\u001B[0m out \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    374\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_optimizer_step_code()\n\u001B[0;32m    376\u001B[0m \u001B[38;5;66;03m# call optimizer step post hooks\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001B[0m, in \u001B[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     74\u001B[0m     torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefaults[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     75\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n\u001B[1;32m---> 76\u001B[0m     ret \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     78\u001B[0m     torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mgraph_break()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:163\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    152\u001B[0m     beta1, beta2 \u001B[38;5;241m=\u001B[39m group[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbetas\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_init_group(\n\u001B[0;32m    155\u001B[0m         group,\n\u001B[0;32m    156\u001B[0m         params_with_grad,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    160\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    161\u001B[0m         state_steps)\n\u001B[1;32m--> 163\u001B[0m     adam(\n\u001B[0;32m    164\u001B[0m         params_with_grad,\n\u001B[0;32m    165\u001B[0m         grads,\n\u001B[0;32m    166\u001B[0m         exp_avgs,\n\u001B[0;32m    167\u001B[0m         exp_avg_sqs,\n\u001B[0;32m    168\u001B[0m         max_exp_avg_sqs,\n\u001B[0;32m    169\u001B[0m         state_steps,\n\u001B[0;32m    170\u001B[0m         amsgrad\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mamsgrad\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    171\u001B[0m         beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[0;32m    172\u001B[0m         beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[0;32m    173\u001B[0m         lr\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    174\u001B[0m         weight_decay\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    175\u001B[0m         eps\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124meps\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    176\u001B[0m         maximize\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    177\u001B[0m         foreach\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mforeach\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    178\u001B[0m         capturable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcapturable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    179\u001B[0m         differentiable\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferentiable\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    180\u001B[0m         fused\u001B[38;5;241m=\u001B[39mgroup[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfused\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[0;32m    181\u001B[0m         grad_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgrad_scale\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    182\u001B[0m         found_inf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfound_inf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    183\u001B[0m     )\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:311\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    309\u001B[0m     func \u001B[38;5;241m=\u001B[39m _single_tensor_adam\n\u001B[1;32m--> 311\u001B[0m func(params,\n\u001B[0;32m    312\u001B[0m      grads,\n\u001B[0;32m    313\u001B[0m      exp_avgs,\n\u001B[0;32m    314\u001B[0m      exp_avg_sqs,\n\u001B[0;32m    315\u001B[0m      max_exp_avg_sqs,\n\u001B[0;32m    316\u001B[0m      state_steps,\n\u001B[0;32m    317\u001B[0m      amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[0;32m    318\u001B[0m      beta1\u001B[38;5;241m=\u001B[39mbeta1,\n\u001B[0;32m    319\u001B[0m      beta2\u001B[38;5;241m=\u001B[39mbeta2,\n\u001B[0;32m    320\u001B[0m      lr\u001B[38;5;241m=\u001B[39mlr,\n\u001B[0;32m    321\u001B[0m      weight_decay\u001B[38;5;241m=\u001B[39mweight_decay,\n\u001B[0;32m    322\u001B[0m      eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[0;32m    323\u001B[0m      maximize\u001B[38;5;241m=\u001B[39mmaximize,\n\u001B[0;32m    324\u001B[0m      capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[0;32m    325\u001B[0m      differentiable\u001B[38;5;241m=\u001B[39mdifferentiable,\n\u001B[0;32m    326\u001B[0m      grad_scale\u001B[38;5;241m=\u001B[39mgrad_scale,\n\u001B[0;32m    327\u001B[0m      found_inf\u001B[38;5;241m=\u001B[39mfound_inf)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:385\u001B[0m, in \u001B[0;36m_single_tensor_adam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001B[0m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;66;03m# Decay the first and second moment running average coefficient\u001B[39;00m\n\u001B[0;32m    384\u001B[0m exp_avg\u001B[38;5;241m.\u001B[39mlerp_(grad, \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta1)\n\u001B[1;32m--> 385\u001B[0m exp_avg_sq\u001B[38;5;241m.\u001B[39mmul_(beta2)\u001B[38;5;241m.\u001B[39maddcmul_(grad, grad\u001B[38;5;241m.\u001B[39mconj(), value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m beta2)\n\u001B[0;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m capturable \u001B[38;5;129;01mor\u001B[39;00m differentiable:\n\u001B[0;32m    388\u001B[0m     step \u001B[38;5;241m=\u001B[39m step_t\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def create_dir(file_path):\n",
    "    if not os.path.exists(os.path.dirname(file_path)):\n",
    "        os.makedirs(os.path.dirname(file_path))\n",
    "\n",
    "\n",
    "def trainer(water_model,water_select_expert,air_model,air_select_expert,train_loader, model, model_save_path, device ,lr, epochs, early_stop_num, verbose=True, writer_flag=False):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    params = model.parameters()\n",
    "    theta1 = model.theta1\n",
    "    theta2 = model.theta2\n",
    "    theta3 = model.theta3\n",
    "    theta4 = model.theta4\n",
    "\n",
    "    water_model=water_model.to(device)\n",
    "    air_model=air_model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params, lr=lr)\n",
    "\n",
    "    create_dir(model_save_path)\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = epochs, math.inf, 0, 0\n",
    "\n",
    "\n",
    "    best_loss1 = best_loss2 = best_loss3 = best_loss4 = math.inf\n",
    "    writer = None\n",
    "    if writer_flag:\n",
    "        writer = SummaryWriter()\n",
    "    ran = range(n_epochs)\n",
    "    if not verbose:\n",
    "        ran = tqdm(range(n_epochs), position=0, leave=True)\n",
    "    for epoch in ran:\n",
    "        model.train()\n",
    "        loss_total_record = []\n",
    "        loss1_record = []\n",
    "        loss2_record = []\n",
    "        loss3_record = []\n",
    "        loss4_record = []\n",
    "\n",
    "        for x, y1, y2, y3, y4 in train_loader:\n",
    "\n",
    "            x, y1, y2, y3, y4 = x.to(device), y1.to(device), y2.to(device), y3.to(device), y4.to(device),\n",
    "\n",
    "\n",
    "            water_expert_represents = water_model(x)[:, water_select_expert, :]\n",
    "            air_expert_represents = air_model(x)[:, air_select_expert, :]\n",
    "            \n",
    "            \n",
    "            pred1, pred2, pred3, pred4 = model(x,water_expert_represents,air_expert_represents)\n",
    "\n",
    "            loss1 = criterion(pred1, y1)\n",
    "            loss2 = criterion(pred2, y2)\n",
    "            loss3 = criterion(pred3, y3)\n",
    "            loss4 = criterion(pred4, y4)\n",
    "\n",
    "\n",
    "            loss_total = loss1 / (theta1 ** 2) +  loss2 / (theta2 ** 2) + loss3 / (theta3 ** 2)*1000 + loss4 / (theta4 ** 2) + 2 * (torch.log(theta1) +torch.log(theta2) + torch.log(theta3) + torch.log(theta4))\n",
    "            optimizer.zero_grad()\n",
    "            loss_total.backward()\n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "\n",
    "            loss_total_record.append(loss_total.detach().item())\n",
    "            loss1_record.append(loss1.detach().item())\n",
    "            loss2_record.append(loss2.detach().item())\n",
    "            loss3_record.append(loss3.detach().item())\n",
    "            loss4_record.append(loss4.detach().item())\n",
    "\n",
    "        mean_train_loss_total = sum(loss_total_record) / len(loss_total_record)\n",
    "        mean_train_loss1 = sum(loss1_record) / len(loss1_record)\n",
    "        mean_train_loss2 = sum(loss2_record) / len(loss2_record)\n",
    "        mean_train_loss3 = sum(loss3_record) / len(loss3_record)\n",
    "        mean_train_loss4 = sum(loss4_record) / len(loss4_record)\n",
    "        if writer_flag:\n",
    "            writer.add_scalar('Loss_total/train', mean_train_loss_total, step)\n",
    "            writer.add_scalar('Loss1/train', mean_train_loss1, step)\n",
    "            writer.add_scalar('Loss2/train', mean_train_loss2, step)\n",
    "            writer.add_scalar('Loss3/train', mean_train_loss3, step)\n",
    "            writer.add_scalar('Loss4/train', mean_train_loss4, step)\n",
    "\n",
    "        if verbose and epoch % 100 == 99:\n",
    "            print(\n",
    "                f'Epoch [{epoch + 1}/{n_epochs}]: Train loss_total: {mean_train_loss_total:.6f}, loss1: {mean_train_loss1:.6f},loss2: {mean_train_loss2:.6f},loss3: {mean_train_loss3:.6f},loss4: {mean_train_loss4:.6f}')\n",
    "\n",
    "        if mean_train_loss_total < best_loss:\n",
    "            best_loss = mean_train_loss_total\n",
    "            best_loss1 = mean_train_loss1\n",
    "            best_loss2 = mean_train_loss2\n",
    "            best_loss3 = mean_train_loss3\n",
    "            best_loss4 = mean_train_loss4\n",
    "\n",
    "            torch.save(model.state_dict(), model_save_path)  # Save your best model\n",
    "            if verbose:\n",
    "                print(\n",
    "                    f\"\\nSave with loss_total: {mean_train_loss_total:.6f}, loss1: {mean_train_loss1:.6f},loss2: {mean_train_loss2:.6f},loss3: {mean_train_loss3:.6f},loss4: {mean_train_loss4:.6f}\")\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= early_stop_num:\n",
    "            print(f'\\nModel is not improving, so we halt the training session at epoch: {epoch + 1}.')\n",
    "            print(\n",
    "                f\"\\n Best Model loss_total: {best_loss:.6f}, loss1: {best_loss1:.6f},loss2: {best_loss2:.6f},loss3: {best_loss3:.6f},loss4: {best_loss4:.6f}\")\n",
    "            print(f\"\\ntheta1:{theta1},theta2:{theta2},theta3:{theta3},theta4:{theta4}\")\n",
    "            return\n",
    "\n",
    "    print(f'\\nTrain all epochs.')\n",
    "    print(\n",
    "        f\"\\n Best Model loss_total: {best_loss:.6f}, loss1: {best_loss1:.6f},loss2: {best_loss2:.6f},loss3: {best_loss3:.6f},loss4: {best_loss4:.6f}\")\n",
    "    print(f\"\\ntheta1:{theta1},theta2:{theta2},theta3:{theta3},theta4:{theta4}\")\n",
    "\n",
    "\n",
    "\n",
    "def predict(model,device,data,y,water_expert_represents_test,air_expert_represents_test):\n",
    "    data = torch.Tensor(data).to(device)\n",
    "    y1 = torch.Tensor(y[:,0]).to(device)\n",
    "    y2 = torch.Tensor(y[:,1]).to(device)\n",
    "    y3 = torch.Tensor(y[:,2]).to(device)\n",
    "    y4 = torch.Tensor(y[:,3]).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss(reduction=\"mean\")\n",
    "    model.eval()\n",
    "    pred1,pred2,pred3,pred4= model(data,water_expert_represents_test,air_expert_represents_test)\n",
    "\n",
    "    pred_list = [pred1,pred2,pred3,pred4]\n",
    "    rmse1  = criterion(pred1,y1).item()**0.5\n",
    "    rmse2  = criterion(pred2,y2).item() ** 0.5\n",
    "    rmse3  = criterion(pred3,y3).item()** 0.5\n",
    "    rmse4  = criterion(pred4,y4).item()** 0.5\n",
    "    rmse_list = [rmse1,rmse2,rmse3,rmse4]\n",
    "\n",
    "    r1 = r2_score(y1.cpu().detach().numpy(),pred1.cpu().detach().numpy())\n",
    "    r2 = r2_score(y2.cpu().detach().numpy(),pred2.cpu().detach().numpy())\n",
    "    r3 = r2_score(y3.cpu().detach().numpy(),pred3.cpu().detach().numpy())\n",
    "    r4 = r2_score(y4.cpu().detach().numpy(),pred4.cpu().detach().numpy())\n",
    "\n",
    "    m1 = mean_absolute_error(y1.cpu().detach().numpy(),pred1.cpu().detach().numpy())\n",
    "    m2 = mean_absolute_error(y2.cpu().detach().numpy(),pred2.cpu().detach().numpy())\n",
    "    m3 = mean_absolute_error(y3.cpu().detach().numpy(),pred3.cpu().detach().numpy())\n",
    "    m4 = mean_absolute_error(y4.cpu().detach().numpy(),pred4.cpu().detach().numpy())\n",
    "\n",
    "    mape1 = mean_absolute_percentage_error(y1.cpu().detach().numpy(),pred1.cpu().detach().numpy())\n",
    "    mape2 = mean_absolute_percentage_error(y2.cpu().detach().numpy(),pred2.cpu().detach().numpy())\n",
    "    mape3 = mean_absolute_percentage_error(y3.cpu().detach().numpy(),pred3.cpu().detach().numpy())\n",
    "    mape4 = mean_absolute_percentage_error(y4.cpu().detach().numpy(),pred4.cpu().detach().numpy())\n",
    "\n",
    "    r_list = [r1,r2,r3,r4]\n",
    "    m_list = [m1,m2,m3,m4]\n",
    "    mape_list = [mape1,mape2,mape3,mape4]\n",
    "    return rmse_list,r_list,m_list,pred_list,mape_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##%% md\n",
    "import numpy as np\n",
    "##%%\n",
    "FILE_DIR = \"../data/process_data\"\n",
    "MODEL_DIR = \"./models\"\n",
    "RESULT_DIR=\"./results\"\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Train')\n",
    "    parser.add_argument('--file_path', type=str, default='data/mutilsolvent2.csv',help='')\n",
    "    parser.add_argument('--model_save_dir', type=str,default=\"./p2results_1\", help='')\n",
    "    parser.add_argument('--res_dir', type=str,default=\"./p2results_1\", help='')\n",
    "\n",
    "    parser.add_argument('--water_model_save_path', type=str,default=\"water_model_result/waterdi6zhe6.ckpt\", help='')\n",
    "    parser.add_argument('--air_model_save_path', default=\"air2_model_result/air2di9zhe6.ckpt\",type=str, help='')\n",
    "    parser.add_argument('--water_select_expert_config', default=\"water_res_result/waterdi6zhe6_top2.csv\",type=str, help='')\n",
    "    parser.add_argument('--air_select_expert_config', type=str, default=\"air2_res_result/air2di9zhe6_top2.csv\",help='')\n",
    "\n",
    "    parser.add_argument('--input_dim', type=int,default=19, help='')\n",
    "    parser.add_argument('--represent_dim', type=int, default=100, help='')\n",
    "    parser.add_argument('--pair_embedding_dim', type=int, default=5, help='')\n",
    "    parser.add_argument('--expert_num', type=int, default=6, help='')\n",
    "    parser.add_argument('--gate_output_dim', type=int, default=10, help='')\n",
    "    parser.add_argument('--epochs',  type=int, default=2000, help='')\n",
    "    parser.add_argument('--early_stop_num',  type=int, default=200, help='')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=0.0005, help='')\n",
    "    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"\")\n",
    "    parser.add_argument(\"--writer_flag\", action=\"store_true\", help=\"\")\n",
    "    parser.add_argument('--batch_size',  type=int, default=16, help='')\n",
    "    parser.add_argument('--test_size', type=float, default=0.2, help='')\n",
    "    parser.add_argument('--seed',  type=int, default=42, help='')\n",
    "    parser.add_argument('--eps', type=str, default=\"\", help='')\n",
    "    parser.add_argument(\"--frozen\", action=\"store_true\", help=\"\")\n",
    "\n",
    "    # device = \"cuda:5\" if torch.cuda.is_available() else \"cpu\"\n",
    "    device = \"cpu\"\n",
    "    karg = parser.parse_args(args=[])\n",
    "    print(karg)\n",
    "    file_name = os.path.basename(karg.file_path)\n",
    "    print(f\"\\n\\n\\n====start to train {file_name}====\")\n",
    "    model_save_path =  os.path.join(karg.model_save_dir,file_name.split(\".\")[0]+\".ckpt\")\n",
    "    print(karg.file_path)\n",
    "    create_dir(model_save_path)\n",
    "\n",
    "    for k in range(0,15):\n",
    "        x_train, x_test, y_train, y_test,transfer_X,transfer_y = get_train_test_data(file_path,karg.test_size,karg.seed,k)\n",
    "\n",
    "        water_model_save_path = karg.water_model_save_path\n",
    "        air_model_save_path = karg.air_model_save_path\n",
    "        model_water = MMoE_Model_Expert()\n",
    "        model_water.load_state_dict(torch.load(water_model_save_path,map_location=torch.device(device)))\n",
    "        model_air = MMoE_Model_Expert()\n",
    "        model_air.load_state_dict(torch.load(air_model_save_path,map_location=torch.device(device)))\n",
    "\n",
    "        if karg.frozen:\n",
    "            model_water.pair_embedding_layer_list.requires_grad_(False)\n",
    "            model_water.expert_list.requires_grad_(False)\n",
    "            model_water.beta_layer.requires_grad_(False)\n",
    "            model_air.pair_embedding_layer_list.requires_grad_(False)\n",
    "            model_air.expert_list.requires_grad_(False)\n",
    "            model_air.beta_layer.requires_grad_(False)\n",
    "\n",
    "\n",
    "\n",
    "        water_top2_index_df  = pd.read_csv(karg.water_select_expert_config)\n",
    "        air_top2_index_df = pd.read_csv(karg.air_select_expert_config)\n",
    "\n",
    "        water_select_expert = torch.Tensor(water_top2_index_df.values).long()\n",
    "        air_select_expert = torch.Tensor(air_top2_index_df.values).long()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        train_dataset = MMoE_Dataset(x_train, y_train[:, 0], y_train[:, 1], y_train[:, 2], y_train[:, 3])\n",
    "        train_dataloader = DataLoader(train_dataset, karg.batch_size, shuffle=True, pin_memory=True)\n",
    "        model = Transfer_Model(input_dim=karg.input_dim, represent_dim=karg.represent_dim, pair_embedding_dim=karg.pair_embedding_dim,\n",
    "                           expert_num=karg.expert_num,gate_output_dim=karg.gate_output_dim).to(device)\n",
    "\n",
    "        trainer(model_water,water_select_expert,model_air,air_select_expert,train_dataloader, model, model_save_path, device, karg.lr, karg.epochs, karg.early_stop_num, karg.verbose, karg.writer_flag)\n",
    "        \n",
    "        water_expert_represents_test = model_water(torch.Tensor(transfer_X).to(device))[:, water_select_expert, :]\n",
    "        air_expert_represents_test = model_air(torch.Tensor(transfer_X).to(device))[:, air_select_expert, :]\n",
    "        rmse, r2, m, _, mape = predict(model, device, transfer_X, transfer_y,water_expert_represents_test,air_expert_represents_test)\n",
    "        print(f\"file:{file_name}, rmse:{rmse}, r2:{r2},mae:{m},mape:{mape}\")\n",
    "        result = pd.DataFrame([rmse + r2 + m + mape],\n",
    "                              columns=[\"eads_rmse\", \"delta_e_rmse\", \"eb_rmse\", \"db_rmse\", \"eads_r2\", \"delta_e_r2\", \"eb_r2\",\n",
    "                                       \"db_r2\", \"eads_mae\",\n",
    "                                       \"delta_e_mae\", \"eb_mae\", \"db_mae\", \"eads_mape\", \"delta_e_mape\", \"eb_mape\",\n",
    "                                       \"db_mape\"])\n",
    "        result_path = os.path.join(karg.res_dir, file_name.split(\".\")[0]+str(k)+\"_transfer.csv\")\n",
    "        create_dir(result_path)\n",
    "        # \n",
    "        result.to_csv(result_path, index_label='num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda363b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad8e43c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e3c9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbd4bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a88a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed0c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a5888e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e93101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14120d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
